{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import datetime\n",
    "import os\n",
    "import gc\n",
    "from collections import namedtuple\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorboard\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import BartConfig, T5Config\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MANUAL_TRAINING = True\n",
    "MANUAL_VALIDATION = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Tokenizer & Model Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kobart_checkpoint = 'gogamza/kobart-base-v2'\n",
    "kot5_checkpoint = 'psyche/KoT5'\n",
    "checkpoint = kobart_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if checkpoint == kobart_checkpoint:\n",
    "    config = BartConfig.from_pretrained(kobart_checkpoint)\n",
    "    #config['vocab'] = 30000\n",
    "else:\n",
    "    config = T5Config.from_pretrained(kot5_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, \n",
    "                                          max_length=512, \n",
    "                                          truncation=False, \n",
    "                                          padding='max_length',\n",
    "                                          #vocab=config.vocab_size\n",
    "                                          )\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(tokenizer) != model.config.vocab_size:\n",
    "    raise RuntimeError(f'Tokenizer vocab size and model vocab size do not match(Tokenizer:{len(tokenizer)} Model: {model.config.vocab_size}). Which would lead to further error in training.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(pd.read_json('data/simplified_data.json'))\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_testvalid = dataset.train_test_split(test_size=0.1)\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5)\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'valid': test_valid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(row):\n",
    "    form_embeddings = tokenizer(row['form'], max_length=512, truncation=True, padding='max_length')\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        correct_form_embeddings = tokenizer(row['corrected_form'], max_length=512, truncation=True, padding='max_length')\n",
    "\n",
    "    return {\n",
    "        'input_ids': form_embeddings['input_ids'],\n",
    "        'attention_mask': form_embeddings['attention_mask'],\n",
    "        'labels': correct_form_embeddings['input_ids'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced_checkpoint = checkpoint.replace('/', '-')\n",
    "tokenized_dataset_path = f'data/{replaced_checkpoint}_tokenized_dataset'\n",
    "\n",
    "if not os.path.exists(tokenized_dataset_path):\n",
    "    tokenized_dataset = (dataset_dict\n",
    "                         .map(tokenize, \n",
    "                              batched=True, \n",
    "                              remove_columns=['form', 'corrected_form'], \n",
    "                              batch_size=512, \n",
    "                              #num_proc=8\n",
    "                              )\n",
    "                         )\n",
    "    \n",
    "    tokenized_dataset.save_to_disk(tokenized_dataset_path)\n",
    "else:\n",
    "    tokenized_dataset = load_dataset(tokenized_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset['train'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise RuntimeError"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"tensorboard\",\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_dict['train'],\n",
    "    eval_dataset=dataset_dict['valid'],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MANUAL_TRAINING:\n",
    "    trainer.train()\n",
    "else:\n",
    "    total_loss_lt = []\n",
    "    batch_loss_lt = []\n",
    "\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    trainset = tokenized_dataset['train'].with_format(\"torch\", device=device)\n",
    "    dataloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "    if not next(model.parameters()).is_cuda and device == torch.device('cuda'):\n",
    "        model.to(device)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(2):\n",
    "        for batch in dataloader:\n",
    "            X = {\n",
    "                    'input_ids': batch['input_ids'],\n",
    "                    'attention_mask': batch['attention_mask'],\n",
    "                }\n",
    "            y = batch['labels']\n",
    "            outputs = model(**X, labels=y)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            batch_loss_lt.append(loss.item())\n",
    "\n",
    "        total_loss_lt += batch_loss_lt\n",
    "        batch_loss_series = pd.Series(batch_loss_lt)\n",
    "        print(f'epoch {epoch + 1} loss: {loss.item()} mean: {batch_loss_series.mean()}')\n",
    "    '''\n",
    "    except:\n",
    "        print(\n",
    "            'input_ids: ' + str(X['input_ids'].shape), \n",
    "            'attention_mask: ' + str(X['attention_mask'].shape), \n",
    "            'labels: ' + str(y.shape), \n",
    "            sep='\\t'\n",
    "        )\n",
    "        '''\n",
    "\n",
    "    total_loss_series = pd.Series(total_loss_lt)\n",
    "    total_loss_series.plot.line()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MANUAL_VALIDATION:\n",
    "    trainer.evaluate(dataset_dict['valid'])\n",
    "else:\n",
    "    loss_lt = []\n",
    "\n",
    "    model.eval()\n",
    "    validset = tokenized_dataset['valid'].with_format(\"torch\", device=device)\n",
    "    dataloader = DataLoader(validset, batch_size=1, shuffle=True)\n",
    "    if not next(model.parameters()).is_cuda and device == torch.device('cuda'):\n",
    "        model.to(device)\n",
    "\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                X = {\n",
    "                        'input_ids': batch['input_ids'],\n",
    "                        'attention_mask': batch['attention_mask'],\n",
    "                    }\n",
    "                y = batch['labels']\n",
    "                outputs = model(**X, labels=y)\n",
    "                loss = outputs.loss\n",
    "                loss_lt.append(loss.item())\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    loss_series = pd.Series(loss_lt)\n",
    "    print(f'loss: {loss_series.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validset = tokenized_dataset['valid'].with_format(\"torch\", device=device)\n",
    "test_sample = validset.shuffle().select(range(1))\n",
    "test_sample_gt = test_sample['labels']\n",
    "test_sample = test_sample.remove_columns('labels')[0]\n",
    "test_sample_input = dict()\n",
    "test_sample_input['input_ids'] = test_sample['input_ids'].unsqueeze(0)\n",
    "test_sample_input['attention_mask'] = test_sample['attention_mask'].unsqueeze(0)\n",
    "output = model.generate(**test_sample_input)\n",
    "input_text = tokenizer.decode(test_sample_input['input_ids'].squeeze(0))\n",
    "output_text = tokenizer.decode(output.squeeze(0))\n",
    "gt_text = tokenizer.decode(test_sample_gt.squeeze(0))\n",
    "\n",
    "print(input_text, output_text, gt_text, sep='\\n\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To prevent unwanted saves\n",
    "raise RuntimeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOW_STR = datetime.datetime.now().strftime('%y%m%d-%H:%M')\n",
    "trainer.create_model_card(\n",
    "    language='Korean',\n",
    "    tags='Grammar',\n",
    "    model='KoGrammar',\n",
    "    finetuned_from=checkpoint\n",
    ")\n",
    "trainer.save_model(f\"./models/{NOW_STR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
