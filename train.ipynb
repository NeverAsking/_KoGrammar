{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import datetime\n",
    "import os\n",
    "import gc\n",
    "from collections import namedtuple\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorboard\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import BartConfig, T5Config\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "MANUAL_TRAINING = True\n",
    "MANUAL_VALIDATION = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Tokenizer & Model Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kobart_checkpoint = 'gogamza/kobart-base-v2'\n",
    "kot5_checkpoint = 'psyche/KoT5'\n",
    "checkpoint = kobart_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "if checkpoint == kobart_checkpoint:\n",
    "    config = BartConfig.from_pretrained(kobart_checkpoint)\n",
    "    #config['vocab'] = 30000\n",
    "else:\n",
    "    config = T5Config.from_pretrained(kot5_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, \n",
    "                                          max_length=512, \n",
    "                                          truncation=True, \n",
    "                                          padding='max_length', \n",
    "                                          #vocab=config.vocab_size\n",
    "                                          )\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 30000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if len(tokenizer) != model.config.vocab_size:\n",
    "    raise RuntimeError('Tokenizer vocab size and model vocab size do not match. Which would lead to further error in training.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1129363"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(pd.read_json('data/simplified_data.json'))\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_testvalid = dataset.train_test_split(test_size=0.1)\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5)\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'valid': test_valid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(row):\n",
    "    form_embeddings = tokenizer(row['form'])\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        correct_form_embeddings = tokenizer(row['corrected_form'])\n",
    "\n",
    "    return {\n",
    "        'input_ids': form_embeddings['input_ids'],\n",
    "        'attention_mask': form_embeddings['attention_mask'],\n",
    "        'labels': correct_form_embeddings['input_ids'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c0bf62765d4e45abc3ba0acdd5d2b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1016426 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527a36334c464bbd9651d97528424d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/56468 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead812bf6a2c42c2a481c7202d040033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/56469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c4c62678d342288c9b10108b6cb813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1016426 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9cbaa253ba64dc8a7027f89414e7be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/56468 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd5123f3bbd14de9b6db2247651d1a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/56469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "replaced_checkpoint = checkpoint.replace('/', '-')\n",
    "tokenized_dataset_path = f'data/{replaced_checkpoint}_tokenized_dataset'\n",
    "\n",
    "if not os.path.exists(tokenized_dataset_path):\n",
    "    tokenized_dataset = (dataset_dict\n",
    "                            .map(tokenize)\n",
    "                            .remove_columns(['form', 'corrected_form'])\n",
    "                        )\n",
    "    tokenized_dataset.save_to_disk(tokenized_dataset_path)\n",
    "else:\n",
    "    tokenized_dataset = load_dataset(tokenized_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__index_level_0__': 630607,\n",
       " 'input_ids': [16875,\n",
       "  14596,\n",
       "  12786,\n",
       "  13590,\n",
       "  14094,\n",
       "  24245,\n",
       "  15425,\n",
       "  10968,\n",
       "  12005,\n",
       "  18570,\n",
       "  14030,\n",
       "  14366,\n",
       "  15610,\n",
       "  13607,\n",
       "  11900,\n",
       "  1271],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'labels': [16875,\n",
       "  14596,\n",
       "  12786,\n",
       "  13590,\n",
       "  14094,\n",
       "  24245,\n",
       "  15425,\n",
       "  29525,\n",
       "  19323,\n",
       "  14366,\n",
       "  15610,\n",
       "  14088,\n",
       "  14543,\n",
       "  1700,\n",
       "  1271]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train'][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"tensorboard\",\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_dict['train'],\n",
    "    eval_dataset=dataset_dict['valid'],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MANUAL_TRAINING:\n",
    "    trainer.train()\n",
    "else:\n",
    "    total_loss_lt = []\n",
    "    batch_loss_lt = []\n",
    "\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    trainset = tokenized_dataset['train'].with_format(\"torch\", device=device)\n",
    "    dataloader = DataLoader(trainset, batch_size=1, shuffle=True)\n",
    "    if not next(model.parameters()).is_cuda and device == torch.device('cuda'):\n",
    "        model.to(device)\n",
    "    \n",
    "    model.train()\n",
    "    try:\n",
    "        for epoch in range(2):\n",
    "            for batch in dataloader:\n",
    "                X = {\n",
    "                        'input_ids': batch['input_ids'],\n",
    "                        'attention_mask': batch['attention_mask'],\n",
    "                    }\n",
    "                y = batch['labels']\n",
    "                outputs = model(**X, labels=y)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                batch_loss_lt.append(loss.item())\n",
    "            total_loss_lt += batch_loss_lt\n",
    "            batch_loss_series = pd.Series(batch_loss_lt)\n",
    "            print(f'epoch {epoch + 1} loss: {loss.item()} mean: {batch_loss_series.mean()}')\n",
    "    except:\n",
    "        print(\n",
    "            'input_ids: ' + str(X['input_ids'].shape), \n",
    "            'attention_mask: ' + str(X['attention_mask'].shape), \n",
    "            'labels: ' + str(y.shape), \n",
    "            sep='\\t'\n",
    "        )\n",
    "\n",
    "    total_loss_series = pd.Series(total_loss_lt)\n",
    "    total_loss_series.plot.line()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.699143689412337\n"
     ]
    }
   ],
   "source": [
    "if not MANUAL_VALIDATION:\n",
    "    trainer.evaluate(dataset_dict['valid'])\n",
    "else:\n",
    "    loss_lt = []\n",
    "\n",
    "    model.eval()\n",
    "    validset = tokenized_dataset['valid'].with_format(\"torch\", device=device)\n",
    "    dataloader = DataLoader(validset, batch_size=1, shuffle=True)\n",
    "    if not next(model.parameters()).is_cuda and device == torch.device('cuda'):\n",
    "        model.to(device)\n",
    "\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                X = {\n",
    "                        'input_ids': batch['input_ids'],\n",
    "                        'attention_mask': batch['attention_mask'],\n",
    "                    }\n",
    "                y = batch['labels']\n",
    "                outputs = model(**X, labels=y)\n",
    "                loss = outputs.loss\n",
    "                loss_lt.append(loss.item())\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    loss_series = pd.Series(loss_lt)\n",
    "    print(f'loss: {loss_series.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validset = tokenized_dataset['valid'].with_format(\"torch\", device=device)\n",
    "test_sample = validset.shuffle().select(range(1))\n",
    "test_sample_gt = test_sample['labels']\n",
    "test_sample = test_sample.remove_columns('labels')[0]\n",
    "test_sample_input = dict()\n",
    "test_sample_input['input_ids'] = test_sample['input_ids'].unsqueeze(0)\n",
    "test_sample_input['attention_mask'] = test_sample['attention_mask'].unsqueeze(0)\n",
    "output = model.generate(**test_sample_input)\n",
    "input_text = tokenizer.decode(test_sample_input['input_ids'].squeeze(0))\n",
    "output_text = tokenizer.decode(output.squeeze(0))\n",
    "gt_text = tokenizer.decode(test_sample_gt.squeeze(0))\n",
    "\n",
    "print(input_text, output_text, gt_text, sep='\\n\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To prevent unwanted saves\n",
    "raise RuntimeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOW_STR = datetime.datetime.now().strftime('%y%m%d-%H:%M')\n",
    "trainer.create_model_card(\n",
    "    language='Korean',\n",
    "    tags='Grammar',\n",
    "    model='KoGrammar',\n",
    "    finetuned_from=checkpoint\n",
    ")\n",
    "trainer.save_model(f\"./models/{NOW_STR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
